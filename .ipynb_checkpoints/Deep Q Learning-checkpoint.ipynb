{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modern Neural Network, adapted from https://github.com/Newmu/Theano-Tutorials\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype = theano.config.floatX)\n",
    "\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \n",
    "    #Simplified RMSprop\n",
    "    def RMSprop(self, cost, weights, a = 0.001, rho = 0.9, eps = 1e-6):\n",
    "        grads = T.grad(cost = cost, wrt = weights)\n",
    "        updates = []\n",
    "        \n",
    "        for w, g in zip(weights, grads):\n",
    "            acc = theano.shared(w.get_value()*0.)\n",
    "            acc_new = rho * acc + (1 - rho) * g**2\n",
    "            grad_scale = T.sqrt(acc_new + eps)\n",
    "            g = g/grad_scale\n",
    "            updates.append((acc, acc_new))\n",
    "            updates.append((p, p - a*g))\n",
    "        \n",
    "        return updates    \n",
    "    \n",
    "    #Random dropout for noise\n",
    "    def dropout(self, X, P = 0.):\n",
    "        if P > 0:\n",
    "            X *= srng.binomial(X.shape, p = (1-P), dtype = theano.config.floatX)\n",
    "            X /= (1-P)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    #Two hidden layer model\n",
    "    def model(self, X, w_h1, w_h2, w_out, P_drop_i, P_drop_h):\n",
    "        X = dropout(X, P_drop_i)\n",
    "        h1 = rectify(T.dot(X, w_h1))\n",
    "        \n",
    "        h1 = dropout(h1, P_drop_h)\n",
    "        h2 = rectify(T.dot(h1, w_h2))\n",
    "        \n",
    "        h2 = dropout(h2, P_drop_h)\n",
    "        py_x = T.dot(h2, w_out)\n",
    "        \n",
    "        return h1, h2, py_x\n",
    "    \n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out, lr, P_drop_i, P_drop_h):\n",
    "        self.X = T.fmatrix()\n",
    "        self.Y = T.fmatrix()\n",
    "        \n",
    "        #Init weights\n",
    "        self.w_h1 = init_weights((n_in, n_h1))\n",
    "        self.w_h2 = init_weights((n_h1, n_h2))\n",
    "        self.w_out = init_weights((n_h2, n_out))\n",
    "    \n",
    "        #Init model\n",
    "        self.noise_h, self.noise_h2, self.noise_py_x = \\\n",
    "        self.model(self.X, self.w_h1, self.w_h2, self.w_out, P_drop_i, P_drop_h)\n",
    "        self.h1, self.h2, self.py_x = \\\n",
    "        self.model(self.X, self.w_h, self.w_h2, self.w_o, 0., 0.)\n",
    "        self.y_x_class = T.argmax(self.py_x, axis=1)\n",
    "        self.y_x_value = T.max(self.py_x, axis=1)\n",
    "        \n",
    "        \n",
    "        #Cost and Updates\n",
    "        self.cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "        self.weights = [self.w_h, self.w_h2, self.w_out]\n",
    "        self.updates = self.RMSprop(self.cost, self.weights, lr=self.lr)\n",
    "        \n",
    "        #Compile to Theano functions\n",
    "        self.train = theano.function(inputs = [self.X, self.Y],\n",
    "                                     outputs = self.cost,\n",
    "                                     updates = self.updates,\n",
    "                                     allow_input_downcast = True)\n",
    "        self.predict_value = theano.function(inputs = [self.X],\n",
    "                                             outputs = self.y_x_value, \n",
    "                                             allow_input_downcast = True)\n",
    "        self.predict_class = theano.function(inputs = [self.X],\n",
    "                                             outputs = self.y_x_class,\n",
    "                                             allow_input_downcast = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-f7eac9ab7032>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-f7eac9ab7032>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    rand_a =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Deep Q Learning\n",
    "import random\n",
    "\n",
    "def state_to_input(state, valid_tile):\n",
    "    state_input = np.flatten(state)\n",
    "    tile_input = np.zeros((19,))\n",
    "    tile_input[valid_tile] = 1\n",
    "    state_input = np.append(state_input, tile_input)\n",
    "    return state_input\n",
    "\n",
    "def output_to_action(output, valid_tile):\n",
    "    tile_idx = output/100\n",
    "    output = output % 100\n",
    "    row = output/10\n",
    "    col = output%10\n",
    "    return (row, col), (valid_tile[tile_idx])\n",
    "\n",
    "class DeepQ(object):\n",
    "    \n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out, lr, P_drop_i, P_drop_h):\n",
    "        self.game = Game()\n",
    "        self.nn = MLP(n_in, n_h1, n_h2, n_out, lr, P_drop_i, P_drop_h)\n",
    "        self.exp = []\n",
    "\n",
    "    def learn(self, n_epoch, exp_len, time_len, eps, gamma):\n",
    "        for e in xrange(n_epoch):\n",
    "            \n",
    "            #Start epoch\n",
    "            score, state, valid_tile, terminal = self.game.start_game()\n",
    "            \n",
    "            for t in xrange(time_len):\n",
    "                \n",
    "                #Epsilon greedy\n",
    "                if(random.random() < self.eps):\n",
    "                    policy_output = random.randint(0,300)\n",
    "                    a_t, tile_t = output_to_action(rand_policy_output, valid_tile)\n",
    "                    \n",
    "                else:\n",
    "                    policy_output = self.nn.predict_class(state_to_input(state, valid_tile))\n",
    "                    a_t, tile_t = output_to_action(on_policy_output, valid_tile)\n",
    "                    \n",
    "                score_t, state_t, valid_tile_t, terminal = self.game.do_action(a_t, tile_t)\n",
    "                reward = (score_t - score)\n",
    "                \n",
    "                #Store Experience\n",
    "                self.exp.append({s:state, \n",
    "                                 vtl:valid_tile, \n",
    "                                 at:a_t, \n",
    "                                 tl:tile, \n",
    "                                 rd:reward,\n",
    "                                 sp:state_t, \n",
    "                                 vtlp:valid_tile_t, \n",
    "                                 tm:terminal})\n",
    "                score = score_t, state = state_t, valid_tile = valid_tile\n",
    "                \n",
    "                #Experience Replay\n",
    "                trX = [], trY = []\n",
    "                if(len(self.exp) > self.exp_len):\n",
    "                    startX = random.randint(0, exp_len - batch_size)\n",
    "                    for xp in xrange(startX, startX + batch_size):\n",
    "                        if(xp[tm] = True):\n",
    "                            Q = reward\n",
    "                        else:\n",
    "                            Q = reward + \\\n",
    "                                gamma * self.nn.predict_value(state_to_input(xp[sp], xp[vtlp]))\n",
    "                                trX.append(state_to_input(xp[s], xp[vtl]))\n",
    "                                trY.apppend([????])\n",
    "                \n",
    "                trX = np.asarray(trX)\n",
    "                trY = np.asarray(trY)\n",
    "                self.nn.train(trX, trY)\n",
    "                \n",
    "                if(terminal == True):\n",
    "                    break\n",
    "    \n",
    "    def play(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
